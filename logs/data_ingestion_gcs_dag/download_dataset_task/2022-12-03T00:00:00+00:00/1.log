[2022-12-04 04:59:14,976] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: data_ingestion_gcs_dag.download_dataset_task scheduled__2022-12-03T00:00:00+00:00 [queued]>
[2022-12-04 04:59:15,033] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: data_ingestion_gcs_dag.download_dataset_task scheduled__2022-12-03T00:00:00+00:00 [queued]>
[2022-12-04 04:59:15,033] {taskinstance.py:1238} INFO - 
--------------------------------------------------------------------------------
[2022-12-04 04:59:15,034] {taskinstance.py:1239} INFO - Starting attempt 1 of 2
[2022-12-04 04:59:15,034] {taskinstance.py:1240} INFO - 
--------------------------------------------------------------------------------
[2022-12-04 04:59:15,084] {taskinstance.py:1259} INFO - Executing <Task(BashOperator): download_dataset_task> on 2022-12-03 00:00:00+00:00
[2022-12-04 04:59:15,103] {standard_task_runner.py:52} INFO - Started process 1484 to run task
[2022-12-04 04:59:15,106] {standard_task_runner.py:76} INFO - Running: ['***', 'tasks', 'run', 'data_ingestion_gcs_dag', 'download_dataset_task', 'scheduled__2022-12-03T00:00:00+00:00', '--job-id', '192', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion_gcs_dag.py', '--cfg-path', '/tmp/tmpfbvis6io', '--error-file', '/tmp/tmp_o9gyz1t']
[2022-12-04 04:59:15,117] {standard_task_runner.py:77} INFO - Job 192: Subtask download_dataset_task
[2022-12-04 04:59:15,296] {logging_mixin.py:109} INFO - Running <TaskInstance: data_ingestion_gcs_dag.download_dataset_task scheduled__2022-12-03T00:00:00+00:00 [running]> on host f96e892f1d4c
[2022-12-04 04:59:15,657] {taskinstance.py:1426} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=data_ingestion_gcs_dag
AIRFLOW_CTX_TASK_ID=download_dataset_task
AIRFLOW_CTX_EXECUTION_DATE=2022-12-03T00:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-12-03T00:00:00+00:00
[2022-12-04 04:59:15,668] {subprocess.py:62} INFO - Tmp dir root location: 
 /tmp
[2022-12-04 04:59:15,669] {subprocess.py:74} INFO - Running command: ['bash', '-c', 'curl -sSL https://datausa.io/api/data?drilldowns=Nation&measures=Population > /opt/***/nation_population.json']
[2022-12-04 04:59:15,769] {subprocess.py:85} INFO - Output:
[2022-12-04 04:59:16,005] {subprocess.py:89} INFO - {"error":"Query must contain at least one measure."}
[2022-12-04 04:59:16,006] {subprocess.py:93} INFO - Command exited with return code 0
[2022-12-04 04:59:16,149] {taskinstance.py:1277} INFO - Marking task as SUCCESS. dag_id=data_ingestion_gcs_dag, task_id=download_dataset_task, execution_date=20221203T000000, start_date=20221204T045914, end_date=20221204T045916
[2022-12-04 04:59:16,299] {local_task_job.py:154} INFO - Task exited with return code 0
[2022-12-04 04:59:16,717] {local_task_job.py:264} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2022-12-04 05:10:34,833] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: data_ingestion_gcs_dag.download_dataset_task scheduled__2022-12-03T00:00:00+00:00 [queued]>
[2022-12-04 05:10:34,851] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: data_ingestion_gcs_dag.download_dataset_task scheduled__2022-12-03T00:00:00+00:00 [queued]>
[2022-12-04 05:10:34,852] {taskinstance.py:1238} INFO - 
--------------------------------------------------------------------------------
[2022-12-04 05:10:34,852] {taskinstance.py:1239} INFO - Starting attempt 1 of 2
[2022-12-04 05:10:34,852] {taskinstance.py:1240} INFO - 
--------------------------------------------------------------------------------
[2022-12-04 05:10:34,874] {taskinstance.py:1259} INFO - Executing <Task(BashOperator): download_dataset_task> on 2022-12-03 00:00:00+00:00
[2022-12-04 05:10:34,881] {standard_task_runner.py:52} INFO - Started process 1878 to run task
[2022-12-04 05:10:34,886] {standard_task_runner.py:76} INFO - Running: ['***', 'tasks', 'run', 'data_ingestion_gcs_dag', 'download_dataset_task', 'scheduled__2022-12-03T00:00:00+00:00', '--job-id', '200', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion_gcs_dag.py', '--cfg-path', '/tmp/tmp2120btom', '--error-file', '/tmp/tmpsz535zo3']
[2022-12-04 05:10:34,887] {standard_task_runner.py:77} INFO - Job 200: Subtask download_dataset_task
[2022-12-04 05:10:34,972] {logging_mixin.py:109} INFO - Running <TaskInstance: data_ingestion_gcs_dag.download_dataset_task scheduled__2022-12-03T00:00:00+00:00 [running]> on host f96e892f1d4c
[2022-12-04 05:10:35,056] {taskinstance.py:1426} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=data_ingestion_gcs_dag
AIRFLOW_CTX_TASK_ID=download_dataset_task
AIRFLOW_CTX_EXECUTION_DATE=2022-12-03T00:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-12-03T00:00:00+00:00
[2022-12-04 05:10:35,058] {subprocess.py:62} INFO - Tmp dir root location: 
 /tmp
[2022-12-04 05:10:35,059] {subprocess.py:74} INFO - Running command: ['bash', '-c', 'curl https://datausa.io/api/data?drilldowns=Nation&measures=Population > /opt/***/nation_population.json']
[2022-12-04 05:10:35,077] {subprocess.py:85} INFO - Output:
[2022-12-04 05:10:35,088] {subprocess.py:89} INFO -   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
[2022-12-04 05:10:35,089] {subprocess.py:89} INFO -                                  Dload  Upload   Total   Spent    Left  Speed
[2022-12-04 05:10:35,251] {subprocess.py:89} INFO -   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100    52  100    52    0     0    317      0 --:--:-- --:--:-- --:--:--   317
[2022-12-04 05:10:35,253] {subprocess.py:89} INFO - {"error":"Query must contain at least one measure."}
[2022-12-04 05:10:35,253] {subprocess.py:93} INFO - Command exited with return code 0
[2022-12-04 05:10:35,309] {taskinstance.py:1277} INFO - Marking task as SUCCESS. dag_id=data_ingestion_gcs_dag, task_id=download_dataset_task, execution_date=20221203T000000, start_date=20221204T051034, end_date=20221204T051035
[2022-12-04 05:10:35,382] {local_task_job.py:154} INFO - Task exited with return code 0
[2022-12-04 05:10:35,517] {local_task_job.py:264} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2022-12-04 05:14:24,939] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: data_ingestion_gcs_dag.download_dataset_task scheduled__2022-12-03T00:00:00+00:00 [queued]>
[2022-12-04 05:14:24,974] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: data_ingestion_gcs_dag.download_dataset_task scheduled__2022-12-03T00:00:00+00:00 [queued]>
[2022-12-04 05:14:24,975] {taskinstance.py:1238} INFO - 
--------------------------------------------------------------------------------
[2022-12-04 05:14:24,975] {taskinstance.py:1239} INFO - Starting attempt 1 of 2
[2022-12-04 05:14:24,975] {taskinstance.py:1240} INFO - 
--------------------------------------------------------------------------------
[2022-12-04 05:14:25,011] {taskinstance.py:1259} INFO - Executing <Task(BashOperator): download_dataset_task> on 2022-12-03 00:00:00+00:00
[2022-12-04 05:14:25,018] {standard_task_runner.py:52} INFO - Started process 2034 to run task
[2022-12-04 05:14:25,025] {standard_task_runner.py:76} INFO - Running: ['***', 'tasks', 'run', 'data_ingestion_gcs_dag', 'download_dataset_task', 'scheduled__2022-12-03T00:00:00+00:00', '--job-id', '208', '--raw', '--subdir', 'DAGS_FOLDER/data_ingest_try_dag.py', '--cfg-path', '/tmp/tmpz987503y', '--error-file', '/tmp/tmp7oujeqa2']
[2022-12-04 05:14:25,026] {standard_task_runner.py:77} INFO - Job 208: Subtask download_dataset_task
[2022-12-04 05:14:25,104] {logging_mixin.py:109} INFO - Running <TaskInstance: data_ingestion_gcs_dag.download_dataset_task scheduled__2022-12-03T00:00:00+00:00 [running]> on host f96e892f1d4c
[2022-12-04 05:14:25,189] {taskinstance.py:1426} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=data_ingestion_gcs_dag
AIRFLOW_CTX_TASK_ID=download_dataset_task
AIRFLOW_CTX_EXECUTION_DATE=2022-12-03T00:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-12-03T00:00:00+00:00
[2022-12-04 05:14:25,191] {subprocess.py:62} INFO - Tmp dir root location: 
 /tmp
[2022-12-04 05:14:25,192] {subprocess.py:74} INFO - Running command: ['bash', '-c', 'curl -X GET https://d37ci6vzurychx.cloudfront.net/misc/taxi+_zone_lookup.csv > /opt/***/taxi+_zone_lookup.csv']
[2022-12-04 05:14:25,208] {subprocess.py:85} INFO - Output:
[2022-12-04 05:14:25,222] {subprocess.py:89} INFO -   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
[2022-12-04 05:14:25,223] {subprocess.py:89} INFO -                                  Dload  Upload   Total   Spent    Left  Speed
[2022-12-04 05:14:25,665] {subprocess.py:89} INFO -   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100 12322  100 12322    0     0  27814      0 --:--:-- --:--:-- --:--:-- 27877
[2022-12-04 05:14:25,668] {subprocess.py:93} INFO - Command exited with return code 0
[2022-12-04 05:14:25,765] {taskinstance.py:1277} INFO - Marking task as SUCCESS. dag_id=data_ingestion_gcs_dag, task_id=download_dataset_task, execution_date=20221203T000000, start_date=20221204T051424, end_date=20221204T051425
[2022-12-04 05:14:25,837] {local_task_job.py:154} INFO - Task exited with return code 0
[2022-12-04 05:14:25,938] {local_task_job.py:264} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2022-12-04 05:30:33,832] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: data_ingestion_gcs_dag.download_dataset_task scheduled__2022-12-03T00:00:00+00:00 [queued]>
[2022-12-04 05:30:33,882] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: data_ingestion_gcs_dag.download_dataset_task scheduled__2022-12-03T00:00:00+00:00 [queued]>
[2022-12-04 05:30:33,883] {taskinstance.py:1238} INFO - 
--------------------------------------------------------------------------------
[2022-12-04 05:30:33,883] {taskinstance.py:1239} INFO - Starting attempt 1 of 2
[2022-12-04 05:30:33,883] {taskinstance.py:1240} INFO - 
--------------------------------------------------------------------------------
[2022-12-04 05:30:33,923] {taskinstance.py:1259} INFO - Executing <Task(PythonOperator): download_dataset_task> on 2022-12-03 00:00:00+00:00
[2022-12-04 05:30:33,943] {standard_task_runner.py:52} INFO - Started process 2587 to run task
[2022-12-04 05:30:33,961] {standard_task_runner.py:76} INFO - Running: ['***', 'tasks', 'run', 'data_ingestion_gcs_dag', 'download_dataset_task', 'scheduled__2022-12-03T00:00:00+00:00', '--job-id', '216', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion_gcs_dag.py', '--cfg-path', '/tmp/tmpikhcjpmb', '--error-file', '/tmp/tmp9gvvv5y9']
[2022-12-04 05:30:33,964] {standard_task_runner.py:77} INFO - Job 216: Subtask download_dataset_task
[2022-12-04 05:30:34,091] {logging_mixin.py:109} INFO - Running <TaskInstance: data_ingestion_gcs_dag.download_dataset_task scheduled__2022-12-03T00:00:00+00:00 [running]> on host f96e892f1d4c
[2022-12-04 05:30:34,254] {taskinstance.py:1426} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=data_ingestion_gcs_dag
AIRFLOW_CTX_TASK_ID=download_dataset_task
AIRFLOW_CTX_EXECUTION_DATE=2022-12-03T00:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-12-03T00:00:00+00:00
[2022-12-04 05:30:34,410] {python.py:175} INFO - Done. Returned value was: None
[2022-12-04 05:30:34,485] {taskinstance.py:1277} INFO - Marking task as SUCCESS. dag_id=data_ingestion_gcs_dag, task_id=download_dataset_task, execution_date=20221203T000000, start_date=20221204T053033, end_date=20221204T053034
[2022-12-04 05:30:34,581] {local_task_job.py:154} INFO - Task exited with return code 0
[2022-12-04 05:30:34,774] {local_task_job.py:264} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2022-12-04 05:40:35,594] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: data_ingestion_gcs_dag.download_dataset_task scheduled__2022-12-03T00:00:00+00:00 [queued]>
[2022-12-04 05:40:35,616] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: data_ingestion_gcs_dag.download_dataset_task scheduled__2022-12-03T00:00:00+00:00 [queued]>
[2022-12-04 05:40:35,617] {taskinstance.py:1238} INFO - 
--------------------------------------------------------------------------------
[2022-12-04 05:40:35,617] {taskinstance.py:1239} INFO - Starting attempt 1 of 2
[2022-12-04 05:40:35,617] {taskinstance.py:1240} INFO - 
--------------------------------------------------------------------------------
[2022-12-04 05:40:35,641] {taskinstance.py:1259} INFO - Executing <Task(PythonOperator): download_dataset_task> on 2022-12-03 00:00:00+00:00
[2022-12-04 05:40:35,648] {standard_task_runner.py:52} INFO - Started process 2935 to run task
[2022-12-04 05:40:35,651] {standard_task_runner.py:76} INFO - Running: ['***', 'tasks', 'run', 'data_ingestion_gcs_dag', 'download_dataset_task', 'scheduled__2022-12-03T00:00:00+00:00', '--job-id', '226', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion_gcs_dag.py', '--cfg-path', '/tmp/tmph00p9wcj', '--error-file', '/tmp/tmpjk_qi_n2']
[2022-12-04 05:40:35,653] {standard_task_runner.py:77} INFO - Job 226: Subtask download_dataset_task
[2022-12-04 05:40:35,726] {logging_mixin.py:109} INFO - Running <TaskInstance: data_ingestion_gcs_dag.download_dataset_task scheduled__2022-12-03T00:00:00+00:00 [running]> on host f96e892f1d4c
[2022-12-04 05:40:35,897] {taskinstance.py:1426} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=data_ingestion_gcs_dag
AIRFLOW_CTX_TASK_ID=download_dataset_task
AIRFLOW_CTX_EXECUTION_DATE=2022-12-03T00:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-12-03T00:00:00+00:00
[2022-12-04 05:40:36,002] {python.py:175} INFO - Done. Returned value was: None
[2022-12-04 05:40:36,071] {taskinstance.py:1277} INFO - Marking task as SUCCESS. dag_id=data_ingestion_gcs_dag, task_id=download_dataset_task, execution_date=20221203T000000, start_date=20221204T054035, end_date=20221204T054036
[2022-12-04 05:40:36,121] {local_task_job.py:154} INFO - Task exited with return code 0
[2022-12-04 05:40:36,220] {local_task_job.py:264} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2022-12-04 05:42:20,499] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: data_ingestion_gcs_dag.download_dataset_task scheduled__2022-12-03T00:00:00+00:00 [queued]>
[2022-12-04 05:42:20,567] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: data_ingestion_gcs_dag.download_dataset_task scheduled__2022-12-03T00:00:00+00:00 [queued]>
[2022-12-04 05:42:20,568] {taskinstance.py:1238} INFO - 
--------------------------------------------------------------------------------
[2022-12-04 05:42:20,569] {taskinstance.py:1239} INFO - Starting attempt 1 of 2
[2022-12-04 05:42:20,579] {taskinstance.py:1240} INFO - 
--------------------------------------------------------------------------------
[2022-12-04 05:42:20,631] {taskinstance.py:1259} INFO - Executing <Task(PythonOperator): download_dataset_task> on 2022-12-03 00:00:00+00:00
[2022-12-04 05:42:20,658] {standard_task_runner.py:52} INFO - Started process 3003 to run task
[2022-12-04 05:42:20,675] {standard_task_runner.py:76} INFO - Running: ['***', 'tasks', 'run', 'data_ingestion_gcs_dag', 'download_dataset_task', 'scheduled__2022-12-03T00:00:00+00:00', '--job-id', '228', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion_gcs_dag.py', '--cfg-path', '/tmp/tmp1gx05spk', '--error-file', '/tmp/tmpd58qmrgt']
[2022-12-04 05:42:20,685] {standard_task_runner.py:77} INFO - Job 228: Subtask download_dataset_task
[2022-12-04 05:42:20,880] {logging_mixin.py:109} INFO - Running <TaskInstance: data_ingestion_gcs_dag.download_dataset_task scheduled__2022-12-03T00:00:00+00:00 [running]> on host f96e892f1d4c
[2022-12-04 05:42:21,148] {taskinstance.py:1426} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=data_ingestion_gcs_dag
AIRFLOW_CTX_TASK_ID=download_dataset_task
AIRFLOW_CTX_EXECUTION_DATE=2022-12-03T00:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-12-03T00:00:00+00:00
[2022-12-04 05:42:21,258] {python.py:175} INFO - Done. Returned value was: None
[2022-12-04 05:42:21,294] {taskinstance.py:1277} INFO - Marking task as SUCCESS. dag_id=data_ingestion_gcs_dag, task_id=download_dataset_task, execution_date=20221203T000000, start_date=20221204T054220, end_date=20221204T054221
[2022-12-04 05:42:21,354] {local_task_job.py:154} INFO - Task exited with return code 0
[2022-12-04 05:42:21,556] {local_task_job.py:264} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2022-12-04 05:45:58,491] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: data_ingestion_gcs_dag.download_dataset_task scheduled__2022-12-03T00:00:00+00:00 [queued]>
[2022-12-04 05:45:58,534] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: data_ingestion_gcs_dag.download_dataset_task scheduled__2022-12-03T00:00:00+00:00 [queued]>
[2022-12-04 05:45:58,534] {taskinstance.py:1238} INFO - 
--------------------------------------------------------------------------------
[2022-12-04 05:45:58,534] {taskinstance.py:1239} INFO - Starting attempt 1 of 2
[2022-12-04 05:45:58,534] {taskinstance.py:1240} INFO - 
--------------------------------------------------------------------------------
[2022-12-04 05:45:58,592] {taskinstance.py:1259} INFO - Executing <Task(PythonOperator): download_dataset_task> on 2022-12-03 00:00:00+00:00
[2022-12-04 05:45:58,600] {standard_task_runner.py:52} INFO - Started process 3120 to run task
[2022-12-04 05:45:58,603] {standard_task_runner.py:76} INFO - Running: ['***', 'tasks', 'run', 'data_ingestion_gcs_dag', 'download_dataset_task', 'scheduled__2022-12-03T00:00:00+00:00', '--job-id', '230', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion_gcs_dag.py', '--cfg-path', '/tmp/tmpvgkn8jvl', '--error-file', '/tmp/tmp4te_ylh8']
[2022-12-04 05:45:58,605] {standard_task_runner.py:77} INFO - Job 230: Subtask download_dataset_task
[2022-12-04 05:45:58,682] {logging_mixin.py:109} INFO - Running <TaskInstance: data_ingestion_gcs_dag.download_dataset_task scheduled__2022-12-03T00:00:00+00:00 [running]> on host f96e892f1d4c
[2022-12-04 05:45:58,769] {taskinstance.py:1426} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=data_ingestion_gcs_dag
AIRFLOW_CTX_TASK_ID=download_dataset_task
AIRFLOW_CTX_EXECUTION_DATE=2022-12-03T00:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-12-03T00:00:00+00:00
[2022-12-04 05:45:58,908] {python.py:175} INFO - Done. Returned value was: None
[2022-12-04 05:45:58,931] {taskinstance.py:1277} INFO - Marking task as SUCCESS. dag_id=data_ingestion_gcs_dag, task_id=download_dataset_task, execution_date=20221203T000000, start_date=20221204T054558, end_date=20221204T054558
[2022-12-04 05:45:58,982] {local_task_job.py:154} INFO - Task exited with return code 0
[2022-12-04 05:45:59,049] {local_task_job.py:264} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2022-12-04 05:53:28,390] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: data_ingestion_gcs_dag.download_dataset_task scheduled__2022-12-03T00:00:00+00:00 [queued]>
[2022-12-04 05:53:28,409] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: data_ingestion_gcs_dag.download_dataset_task scheduled__2022-12-03T00:00:00+00:00 [queued]>
[2022-12-04 05:53:28,410] {taskinstance.py:1238} INFO - 
--------------------------------------------------------------------------------
[2022-12-04 05:53:28,410] {taskinstance.py:1239} INFO - Starting attempt 1 of 2
[2022-12-04 05:53:28,410] {taskinstance.py:1240} INFO - 
--------------------------------------------------------------------------------
[2022-12-04 05:53:28,431] {taskinstance.py:1259} INFO - Executing <Task(PythonOperator): download_dataset_task> on 2022-12-03 00:00:00+00:00
[2022-12-04 05:53:28,438] {standard_task_runner.py:52} INFO - Started process 3392 to run task
[2022-12-04 05:53:28,442] {standard_task_runner.py:76} INFO - Running: ['***', 'tasks', 'run', 'data_ingestion_gcs_dag', 'download_dataset_task', 'scheduled__2022-12-03T00:00:00+00:00', '--job-id', '240', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion_gcs_dag.py', '--cfg-path', '/tmp/tmp15y92eyp', '--error-file', '/tmp/tmpvvj7g8w5']
[2022-12-04 05:53:28,443] {standard_task_runner.py:77} INFO - Job 240: Subtask download_dataset_task
[2022-12-04 05:53:28,508] {logging_mixin.py:109} INFO - Running <TaskInstance: data_ingestion_gcs_dag.download_dataset_task scheduled__2022-12-03T00:00:00+00:00 [running]> on host f96e892f1d4c
[2022-12-04 05:53:28,579] {taskinstance.py:1426} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=data_ingestion_gcs_dag
AIRFLOW_CTX_TASK_ID=download_dataset_task
AIRFLOW_CTX_EXECUTION_DATE=2022-12-03T00:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-12-03T00:00:00+00:00
[2022-12-04 05:53:28,738] {python.py:175} INFO - Done. Returned value was: None
[2022-12-04 05:53:28,754] {taskinstance.py:1277} INFO - Marking task as SUCCESS. dag_id=data_ingestion_gcs_dag, task_id=download_dataset_task, execution_date=20221203T000000, start_date=20221204T055328, end_date=20221204T055328
[2022-12-04 05:53:28,817] {local_task_job.py:154} INFO - Task exited with return code 0
[2022-12-04 05:53:28,875] {local_task_job.py:264} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2022-12-04 05:57:31,366] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: data_ingestion_gcs_dag.download_dataset_task scheduled__2022-12-03T00:00:00+00:00 [queued]>
[2022-12-04 05:57:31,387] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: data_ingestion_gcs_dag.download_dataset_task scheduled__2022-12-03T00:00:00+00:00 [queued]>
[2022-12-04 05:57:31,387] {taskinstance.py:1238} INFO - 
--------------------------------------------------------------------------------
[2022-12-04 05:57:31,387] {taskinstance.py:1239} INFO - Starting attempt 1 of 2
[2022-12-04 05:57:31,387] {taskinstance.py:1240} INFO - 
--------------------------------------------------------------------------------
[2022-12-04 05:57:31,414] {taskinstance.py:1259} INFO - Executing <Task(PythonOperator): download_dataset_task> on 2022-12-03 00:00:00+00:00
[2022-12-04 05:57:31,425] {standard_task_runner.py:52} INFO - Started process 3554 to run task
[2022-12-04 05:57:31,428] {standard_task_runner.py:76} INFO - Running: ['***', 'tasks', 'run', 'data_ingestion_gcs_dag', 'download_dataset_task', 'scheduled__2022-12-03T00:00:00+00:00', '--job-id', '250', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion_gcs_dag.py', '--cfg-path', '/tmp/tmpfakie7to', '--error-file', '/tmp/tmp8dv5ey7x']
[2022-12-04 05:57:31,429] {standard_task_runner.py:77} INFO - Job 250: Subtask download_dataset_task
[2022-12-04 05:57:31,513] {logging_mixin.py:109} INFO - Running <TaskInstance: data_ingestion_gcs_dag.download_dataset_task scheduled__2022-12-03T00:00:00+00:00 [running]> on host f96e892f1d4c
[2022-12-04 05:57:31,617] {taskinstance.py:1426} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=data_ingestion_gcs_dag
AIRFLOW_CTX_TASK_ID=download_dataset_task
AIRFLOW_CTX_EXECUTION_DATE=2022-12-03T00:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-12-03T00:00:00+00:00
[2022-12-04 05:57:31,695] {python.py:175} INFO - Done. Returned value was: None
[2022-12-04 05:57:31,718] {taskinstance.py:1277} INFO - Marking task as SUCCESS. dag_id=data_ingestion_gcs_dag, task_id=download_dataset_task, execution_date=20221203T000000, start_date=20221204T055731, end_date=20221204T055731
[2022-12-04 05:57:31,773] {local_task_job.py:154} INFO - Task exited with return code 0
[2022-12-04 05:57:31,856] {local_task_job.py:264} INFO - 1 downstream tasks scheduled from follow-on schedule check
